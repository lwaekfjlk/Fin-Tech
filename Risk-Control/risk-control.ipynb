{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhaofei/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_new.csv', 'test_new.csv', 'feature_x.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X65</th>\n",
       "      <th>X66</th>\n",
       "      <th>X67</th>\n",
       "      <th>X68</th>\n",
       "      <th>X69</th>\n",
       "      <th>X70</th>\n",
       "      <th>X71</th>\n",
       "      <th>X72</th>\n",
       "      <th>Y</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>17147.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>2449.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12990.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18710.0</td>\n",
       "      <td>...</td>\n",
       "      <td>230.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1398.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19010.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16410.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1      X2       X3    X4   X5       X6   X7  X8   X9      X10  ...  \\\n",
       "0   9.0  1458.0  17147.0  10.0  0.0    800.0  0.0 NaN  0.0    679.0  ...   \n",
       "1   2.0   250.0     38.0   6.0  NaN  10000.0  0.0 NaN  1.0  12990.0  ...   \n",
       "2   2.0  1054.0    178.0   1.0  0.0   1000.0  0.0 NaN  1.0  18710.0  ...   \n",
       "3  10.0  1398.0    679.0   7.0  0.0  10000.0  0.0 NaN  1.0  19010.0  ...   \n",
       "4   2.0  1095.0    305.0  11.0  0.0  10000.0  0.0 NaN  2.0  16410.0  ...   \n",
       "\n",
       "     X65    X66     X67    X68    X69  X70  X71  X72  Y  id  \n",
       "0    7.0  581.0  2449.0   93.0  498.0  6.0  0.0  0.0  1   0  \n",
       "1   31.0  796.0     7.0  122.0  406.0  5.0  NaN  NaN  1   1  \n",
       "2  230.0  732.0    29.0   78.0   10.0  6.0  0.0  0.0  0   2  \n",
       "3   11.0   36.0   113.0   82.0   35.0  6.0  0.0  0.0  1   3  \n",
       "4   93.0  395.0    50.0   48.0  491.0  5.0  0.0  0.0  0   4  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train = pd.read_csv('./data/train_new.csv')\n",
    "train_labels = app_train['Y']\n",
    "app_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X64</th>\n",
       "      <th>X65</th>\n",
       "      <th>X66</th>\n",
       "      <th>X67</th>\n",
       "      <th>X68</th>\n",
       "      <th>X69</th>\n",
       "      <th>X70</th>\n",
       "      <th>X71</th>\n",
       "      <th>X72</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4355.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4083.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3072000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14330.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>766.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>34571438.0</td>\n",
       "      <td>8.333504e+09</td>\n",
       "      <td>50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25350.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9054.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>50002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2038.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1      X2      X3    X4            X5       X6   X7         X8   X9  \\\n",
       "0  3.0   853.0   208.0  15.0  0.000000e+00   1000.0  0.0        NaN  2.0   \n",
       "1  2.0  1637.0    84.0  22.0  1.000000e+10  10000.0  0.0  3072000.0  4.0   \n",
       "2  8.0  3120.0  1218.0  30.0  0.000000e+00  30000.0  0.0        NaN  0.0   \n",
       "3  6.0  1938.0    48.0  14.0           NaN   1000.0  0.0        NaN  0.0   \n",
       "4  NaN     NaN     NaN   NaN           NaN      NaN  NaN        NaN  NaN   \n",
       "\n",
       "       X10  ...     X64    X65    X66    X67    X68     X69  X70         X71  \\\n",
       "0      0.0  ...  4355.0  168.0  238.0   34.0   78.0  4083.0  6.0         0.0   \n",
       "1  14330.0  ...  1974.0  476.0  766.0   14.0   60.0  1252.0  6.0  34571438.0   \n",
       "2  25350.0  ... -9054.0  293.0  135.0  203.0   64.0   231.0  6.0         0.0   \n",
       "3  32126.0  ...  2038.0  388.0  155.0    9.0  100.0   317.0  5.0         NaN   \n",
       "4      NaN  ...     NaN    NaN    NaN    NaN    NaN     NaN  NaN         NaN   \n",
       "\n",
       "            X72     id  \n",
       "0  0.000000e+00  50000  \n",
       "1  8.333504e+09  50001  \n",
       "2  0.000000e+00  50002  \n",
       "3           NaN  50003  \n",
       "4           NaN  50004  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_test = pd.read_csv('./data/test_new.csv')\n",
    "final_test = app_test\n",
    "app_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Y', 1.0),\n",
       " ('X27', -0.2441447948441628),\n",
       " ('X30', -0.1644313669527459),\n",
       " ('X42', -0.1515420651706445),\n",
       " ('X25', -0.14620849738853522),\n",
       " ('X22', 0.14520811328899136),\n",
       " ('X34', 0.1412437064881161),\n",
       " ('X16', -0.14030661126550095),\n",
       " ('X14', -0.1289838932933782),\n",
       " ('X69', -0.11988376089532285)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose most important features for feature construction\n",
    "new_corrs = []\n",
    "columns = app_train.columns.values.tolist()\n",
    "# Iterate through the columns \n",
    "for col in columns:\n",
    "    # Calculate correlation with the target\n",
    "    corr = app_train['Y'].corr(app_train[col])\n",
    "\n",
    "    # Append the list as a tuple\n",
    "    new_corrs.append((col, corr))\n",
    "new_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\n",
    "new_corrs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Features shape:  (50000, 495)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'X27',\n",
       " 'X30',\n",
       " 'X42',\n",
       " 'X25',\n",
       " 'X27^2',\n",
       " 'X27 X30',\n",
       " 'X27 X42',\n",
       " 'X27 X25',\n",
       " 'X30^2',\n",
       " 'X30 X42',\n",
       " 'X30 X25',\n",
       " 'X42^2',\n",
       " 'X42 X25',\n",
       " 'X25^2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "# do poly feature engineering using combination of 5 features\n",
    "poly_features_train = app_train[['X27', 'X30', 'X42', 'X25', 'Y']]\n",
    "poly_features_test = app_test[['X27', 'X30', 'X42', 'X25']]\n",
    "\n",
    "poly_target = poly_features_train['Y']\n",
    "poly_features_train = poly_features_train.drop(columns = ['Y'])\n",
    "\n",
    "# fill NaN in the table\n",
    "imputer = Imputer(strategy = 'median')\n",
    "poly_features_train = imputer.fit_transform(poly_features_train)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "                        \n",
    "poly_transformer = PolynomialFeatures(degree = 8)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features_train)\n",
    "\n",
    "# Transform the features\n",
    "poly_features_train = poly_transformer.transform(poly_features_train)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features_train.shape)\n",
    "\n",
    "poly_transformer.get_feature_names(input_features = ['X27', 'X30', 'X42', 'X25'])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data with polynomial features shape:  (50000, 564)\n",
      "Testing data with polynomial features shape:   (10000, 564)\n"
     ]
    }
   ],
   "source": [
    "poly_features_train = pd.DataFrame(poly_features_train, \n",
    "                             columns = poly_transformer.get_feature_names(['X27', 'X30', 'X42', 'X25']))\n",
    "poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                                  columns = poly_transformer.get_feature_names(['X27', 'X30', 'X42', 'X25']))\n",
    "\n",
    "poly_features_train['Y'] = poly_target\n",
    "\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features_train['id'] = app_train['id']\n",
    "app_train_poly = app_train.merge(poly_features_train, how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['id'] = app_test['id']\n",
    "app_test_poly = app_test.merge(poly_features_test, how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test_poly.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X10', 0.0004681548170859026),\n",
       " ('X64', -0.0005019570755814007),\n",
       " ('X56', -0.0009662858716957663),\n",
       " ('X35', -0.0010872158022036766),\n",
       " ('X31', 0.0011067876254034932),\n",
       " ('X36', 0.0013384845715644952),\n",
       " ('X18', 0.003370143152067094),\n",
       " ('X48', -0.0045215086031005),\n",
       " ('X72', -0.004945275360387715),\n",
       " ('X71', -0.0050328634531733646),\n",
       " ('id', -0.005236023054915896),\n",
       " ('X11', 0.005342887167242225),\n",
       " ('X51', -0.006143902052987893),\n",
       " ('X60', -0.006717669802731037),\n",
       " ('X23', -0.006749875178364244),\n",
       " ('X47', 0.007373489907406655),\n",
       " ('X1', -0.007396490969991991),\n",
       " ('X5', -0.008345795692111219),\n",
       " ('X6', -0.009835351281528345),\n",
       " ('X68', 0.010414693741793378)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose most important features after feature construction\n",
    "new_corrs = []\n",
    "columns = app_train_poly.columns.values.tolist()\n",
    "# Iterate through the columns \n",
    "for col in columns:\n",
    "    # Calculate correlation with the target\n",
    "    corr = poly_target.corr(app_train_poly[col])\n",
    "\n",
    "    # Append the list as a tuple\n",
    "    new_corrs.append((col, corr))\n",
    "new_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = False)\n",
    "new_corrs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 464), (10000, 464))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    app_train_poly = app_train_poly.drop(columns = [new_corrs[i][0]])\n",
    "    app_test_poly = app_test_poly.drop(columns = [new_corrs[i][0]])\n",
    "\n",
    "app_train = app_train_poly\n",
    "app_test = app_test_poly\n",
    "app_train.shape,app_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train = app_train\n",
    "test  = app_test\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "imputer = Imputer(strategy = 'median')\n",
    "imputer.fit(train)\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 464)\n",
      "(10000, 464)\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train)\n",
    "test  = np.array(test)\n",
    "train_labels = np.array(train_labels)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def evaluate_AUC(y_pred,y_true):\n",
    "    print(roc_auc_score(y_true,y_pred))\n",
    "\n",
    "def get_train_test_dataset(total_train,total_train_labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(total_train, total_train_labels, test_size=0.3, random_state=31)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_dataset(train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.676129\tvalid_1's auc: 0.658628\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's auc: 0.69063\tvalid_1's auc: 0.667332\n",
      "[3]\ttraining's auc: 0.695916\tvalid_1's auc: 0.672061\n",
      "[4]\ttraining's auc: 0.699986\tvalid_1's auc: 0.674542\n",
      "[5]\ttraining's auc: 0.703838\tvalid_1's auc: 0.677911\n",
      "[6]\ttraining's auc: 0.705333\tvalid_1's auc: 0.679055\n",
      "[7]\ttraining's auc: 0.706751\tvalid_1's auc: 0.679477\n",
      "[8]\ttraining's auc: 0.708203\tvalid_1's auc: 0.681024\n",
      "[9]\ttraining's auc: 0.709686\tvalid_1's auc: 0.682295\n",
      "[10]\ttraining's auc: 0.710587\tvalid_1's auc: 0.683096\n",
      "[11]\ttraining's auc: 0.712287\tvalid_1's auc: 0.684251\n",
      "[12]\ttraining's auc: 0.713648\tvalid_1's auc: 0.685594\n",
      "[13]\ttraining's auc: 0.715225\tvalid_1's auc: 0.686699\n",
      "[14]\ttraining's auc: 0.717338\tvalid_1's auc: 0.68852\n",
      "[15]\ttraining's auc: 0.718598\tvalid_1's auc: 0.689471\n",
      "[16]\ttraining's auc: 0.720085\tvalid_1's auc: 0.690117\n",
      "[17]\ttraining's auc: 0.721395\tvalid_1's auc: 0.691375\n",
      "[18]\ttraining's auc: 0.722539\tvalid_1's auc: 0.692458\n",
      "[19]\ttraining's auc: 0.723741\tvalid_1's auc: 0.693357\n",
      "[20]\ttraining's auc: 0.724911\tvalid_1's auc: 0.693775\n",
      "[21]\ttraining's auc: 0.725862\tvalid_1's auc: 0.693936\n",
      "[22]\ttraining's auc: 0.727012\tvalid_1's auc: 0.694505\n",
      "[23]\ttraining's auc: 0.728171\tvalid_1's auc: 0.695526\n",
      "[24]\ttraining's auc: 0.729184\tvalid_1's auc: 0.696313\n",
      "[25]\ttraining's auc: 0.729971\tvalid_1's auc: 0.697141\n",
      "[26]\ttraining's auc: 0.730822\tvalid_1's auc: 0.69775\n",
      "[27]\ttraining's auc: 0.732057\tvalid_1's auc: 0.698695\n",
      "[28]\ttraining's auc: 0.733035\tvalid_1's auc: 0.699539\n",
      "[29]\ttraining's auc: 0.733915\tvalid_1's auc: 0.700075\n",
      "[30]\ttraining's auc: 0.734873\tvalid_1's auc: 0.70096\n",
      "[31]\ttraining's auc: 0.735899\tvalid_1's auc: 0.701992\n",
      "[32]\ttraining's auc: 0.73682\tvalid_1's auc: 0.70218\n",
      "[33]\ttraining's auc: 0.737667\tvalid_1's auc: 0.702667\n",
      "[34]\ttraining's auc: 0.738341\tvalid_1's auc: 0.703248\n",
      "[35]\ttraining's auc: 0.739313\tvalid_1's auc: 0.703967\n",
      "[36]\ttraining's auc: 0.740185\tvalid_1's auc: 0.704586\n",
      "[37]\ttraining's auc: 0.741037\tvalid_1's auc: 0.705137\n",
      "[38]\ttraining's auc: 0.742133\tvalid_1's auc: 0.705663\n",
      "[39]\ttraining's auc: 0.742938\tvalid_1's auc: 0.706032\n",
      "[40]\ttraining's auc: 0.743577\tvalid_1's auc: 0.70657\n",
      "[41]\ttraining's auc: 0.74417\tvalid_1's auc: 0.707171\n",
      "[42]\ttraining's auc: 0.744988\tvalid_1's auc: 0.707851\n",
      "[43]\ttraining's auc: 0.745646\tvalid_1's auc: 0.70813\n",
      "[44]\ttraining's auc: 0.746273\tvalid_1's auc: 0.708298\n",
      "[45]\ttraining's auc: 0.746966\tvalid_1's auc: 0.708717\n",
      "[46]\ttraining's auc: 0.747899\tvalid_1's auc: 0.709052\n",
      "[47]\ttraining's auc: 0.748591\tvalid_1's auc: 0.709295\n",
      "[48]\ttraining's auc: 0.749645\tvalid_1's auc: 0.710093\n",
      "[49]\ttraining's auc: 0.750292\tvalid_1's auc: 0.710469\n",
      "[50]\ttraining's auc: 0.751052\tvalid_1's auc: 0.710842\n",
      "[51]\ttraining's auc: 0.751731\tvalid_1's auc: 0.711183\n",
      "[52]\ttraining's auc: 0.752743\tvalid_1's auc: 0.711976\n",
      "[53]\ttraining's auc: 0.753395\tvalid_1's auc: 0.712101\n",
      "[54]\ttraining's auc: 0.754133\tvalid_1's auc: 0.71256\n",
      "[55]\ttraining's auc: 0.754764\tvalid_1's auc: 0.712745\n",
      "[56]\ttraining's auc: 0.755425\tvalid_1's auc: 0.712871\n",
      "[57]\ttraining's auc: 0.755954\tvalid_1's auc: 0.71302\n",
      "[58]\ttraining's auc: 0.756528\tvalid_1's auc: 0.713359\n",
      "[59]\ttraining's auc: 0.757056\tvalid_1's auc: 0.713564\n",
      "[60]\ttraining's auc: 0.757469\tvalid_1's auc: 0.713738\n",
      "[61]\ttraining's auc: 0.757995\tvalid_1's auc: 0.714004\n",
      "[62]\ttraining's auc: 0.758629\tvalid_1's auc: 0.714333\n",
      "[63]\ttraining's auc: 0.759188\tvalid_1's auc: 0.714692\n",
      "[64]\ttraining's auc: 0.759909\tvalid_1's auc: 0.715004\n",
      "[65]\ttraining's auc: 0.760703\tvalid_1's auc: 0.715263\n",
      "[66]\ttraining's auc: 0.761326\tvalid_1's auc: 0.715525\n",
      "[67]\ttraining's auc: 0.761932\tvalid_1's auc: 0.715809\n",
      "[68]\ttraining's auc: 0.762467\tvalid_1's auc: 0.716143\n",
      "[69]\ttraining's auc: 0.763039\tvalid_1's auc: 0.716229\n",
      "[70]\ttraining's auc: 0.763581\tvalid_1's auc: 0.716506\n",
      "[71]\ttraining's auc: 0.764166\tvalid_1's auc: 0.716617\n",
      "[72]\ttraining's auc: 0.764722\tvalid_1's auc: 0.717092\n",
      "[73]\ttraining's auc: 0.765144\tvalid_1's auc: 0.717092\n",
      "[74]\ttraining's auc: 0.76577\tvalid_1's auc: 0.717291\n",
      "[75]\ttraining's auc: 0.766199\tvalid_1's auc: 0.717505\n",
      "[76]\ttraining's auc: 0.766584\tvalid_1's auc: 0.717884\n",
      "[77]\ttraining's auc: 0.767125\tvalid_1's auc: 0.718301\n",
      "[78]\ttraining's auc: 0.767854\tvalid_1's auc: 0.718535\n",
      "[79]\ttraining's auc: 0.768475\tvalid_1's auc: 0.718509\n",
      "[80]\ttraining's auc: 0.768922\tvalid_1's auc: 0.718596\n",
      "[81]\ttraining's auc: 0.769424\tvalid_1's auc: 0.718627\n",
      "[82]\ttraining's auc: 0.770133\tvalid_1's auc: 0.718656\n",
      "[83]\ttraining's auc: 0.770558\tvalid_1's auc: 0.718719\n",
      "[84]\ttraining's auc: 0.77103\tvalid_1's auc: 0.718974\n",
      "[85]\ttraining's auc: 0.771463\tvalid_1's auc: 0.719093\n",
      "[86]\ttraining's auc: 0.771926\tvalid_1's auc: 0.719115\n",
      "[87]\ttraining's auc: 0.77248\tvalid_1's auc: 0.719384\n",
      "[88]\ttraining's auc: 0.772833\tvalid_1's auc: 0.719342\n",
      "[89]\ttraining's auc: 0.77323\tvalid_1's auc: 0.719422\n",
      "[90]\ttraining's auc: 0.773773\tvalid_1's auc: 0.719593\n",
      "[91]\ttraining's auc: 0.774203\tvalid_1's auc: 0.719687\n",
      "[92]\ttraining's auc: 0.774529\tvalid_1's auc: 0.719676\n",
      "[93]\ttraining's auc: 0.774973\tvalid_1's auc: 0.719779\n",
      "[94]\ttraining's auc: 0.775353\tvalid_1's auc: 0.719844\n",
      "[95]\ttraining's auc: 0.775872\tvalid_1's auc: 0.719804\n",
      "[96]\ttraining's auc: 0.776312\tvalid_1's auc: 0.719849\n",
      "[97]\ttraining's auc: 0.776941\tvalid_1's auc: 0.720365\n",
      "[98]\ttraining's auc: 0.777231\tvalid_1's auc: 0.720262\n",
      "[99]\ttraining's auc: 0.777695\tvalid_1's auc: 0.720446\n",
      "[100]\ttraining's auc: 0.778194\tvalid_1's auc: 0.72059\n",
      "[101]\ttraining's auc: 0.778751\tvalid_1's auc: 0.720742\n",
      "[102]\ttraining's auc: 0.779174\tvalid_1's auc: 0.72088\n",
      "[103]\ttraining's auc: 0.779763\tvalid_1's auc: 0.721176\n",
      "[104]\ttraining's auc: 0.780341\tvalid_1's auc: 0.721241\n",
      "[105]\ttraining's auc: 0.780679\tvalid_1's auc: 0.721212\n",
      "[106]\ttraining's auc: 0.781064\tvalid_1's auc: 0.721304\n",
      "[107]\ttraining's auc: 0.781511\tvalid_1's auc: 0.72158\n",
      "[108]\ttraining's auc: 0.781909\tvalid_1's auc: 0.721711\n",
      "[109]\ttraining's auc: 0.782392\tvalid_1's auc: 0.721823\n",
      "[110]\ttraining's auc: 0.782913\tvalid_1's auc: 0.722028\n",
      "[111]\ttraining's auc: 0.783319\tvalid_1's auc: 0.721997\n",
      "[112]\ttraining's auc: 0.783767\tvalid_1's auc: 0.722221\n",
      "[113]\ttraining's auc: 0.784032\tvalid_1's auc: 0.722313\n",
      "[114]\ttraining's auc: 0.784492\tvalid_1's auc: 0.722518\n",
      "[115]\ttraining's auc: 0.784872\tvalid_1's auc: 0.722675\n",
      "[116]\ttraining's auc: 0.785232\tvalid_1's auc: 0.722648\n",
      "[117]\ttraining's auc: 0.785656\tvalid_1's auc: 0.722698\n",
      "[118]\ttraining's auc: 0.786227\tvalid_1's auc: 0.722699\n",
      "[119]\ttraining's auc: 0.786558\tvalid_1's auc: 0.722712\n",
      "[120]\ttraining's auc: 0.786956\tvalid_1's auc: 0.722754\n",
      "[121]\ttraining's auc: 0.787404\tvalid_1's auc: 0.722813\n",
      "[122]\ttraining's auc: 0.787808\tvalid_1's auc: 0.722819\n",
      "[123]\ttraining's auc: 0.788186\tvalid_1's auc: 0.723009\n",
      "[124]\ttraining's auc: 0.788536\tvalid_1's auc: 0.7231\n",
      "[125]\ttraining's auc: 0.788764\tvalid_1's auc: 0.723072\n",
      "[126]\ttraining's auc: 0.789141\tvalid_1's auc: 0.723341\n",
      "[127]\ttraining's auc: 0.789452\tvalid_1's auc: 0.723326\n",
      "[128]\ttraining's auc: 0.790047\tvalid_1's auc: 0.723484\n",
      "[129]\ttraining's auc: 0.790271\tvalid_1's auc: 0.723613\n",
      "[130]\ttraining's auc: 0.790783\tvalid_1's auc: 0.723798\n",
      "[131]\ttraining's auc: 0.791168\tvalid_1's auc: 0.723835\n",
      "[132]\ttraining's auc: 0.791624\tvalid_1's auc: 0.723789\n",
      "[133]\ttraining's auc: 0.792167\tvalid_1's auc: 0.723814\n",
      "[134]\ttraining's auc: 0.792568\tvalid_1's auc: 0.723813\n",
      "[135]\ttraining's auc: 0.793071\tvalid_1's auc: 0.72392\n",
      "[136]\ttraining's auc: 0.793473\tvalid_1's auc: 0.723911\n",
      "[137]\ttraining's auc: 0.793782\tvalid_1's auc: 0.723967\n",
      "[138]\ttraining's auc: 0.794167\tvalid_1's auc: 0.724011\n",
      "[139]\ttraining's auc: 0.79446\tvalid_1's auc: 0.724034\n",
      "[140]\ttraining's auc: 0.794865\tvalid_1's auc: 0.724192\n",
      "[141]\ttraining's auc: 0.795284\tvalid_1's auc: 0.72421\n",
      "[142]\ttraining's auc: 0.795589\tvalid_1's auc: 0.724256\n",
      "[143]\ttraining's auc: 0.796019\tvalid_1's auc: 0.724277\n",
      "[144]\ttraining's auc: 0.796403\tvalid_1's auc: 0.724316\n",
      "[145]\ttraining's auc: 0.796847\tvalid_1's auc: 0.724238\n",
      "[146]\ttraining's auc: 0.797115\tvalid_1's auc: 0.72423\n",
      "[147]\ttraining's auc: 0.797635\tvalid_1's auc: 0.724517\n",
      "[148]\ttraining's auc: 0.798112\tvalid_1's auc: 0.724531\n",
      "[149]\ttraining's auc: 0.798416\tvalid_1's auc: 0.72445\n",
      "[150]\ttraining's auc: 0.798726\tvalid_1's auc: 0.724477\n",
      "[151]\ttraining's auc: 0.79911\tvalid_1's auc: 0.724496\n",
      "[152]\ttraining's auc: 0.799341\tvalid_1's auc: 0.724453\n",
      "[153]\ttraining's auc: 0.799701\tvalid_1's auc: 0.724544\n",
      "[154]\ttraining's auc: 0.799997\tvalid_1's auc: 0.724507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155]\ttraining's auc: 0.800305\tvalid_1's auc: 0.72456\n",
      "[156]\ttraining's auc: 0.800593\tvalid_1's auc: 0.724649\n",
      "[157]\ttraining's auc: 0.800893\tvalid_1's auc: 0.724637\n",
      "[158]\ttraining's auc: 0.801226\tvalid_1's auc: 0.724615\n",
      "[159]\ttraining's auc: 0.8014\tvalid_1's auc: 0.724526\n",
      "[160]\ttraining's auc: 0.801612\tvalid_1's auc: 0.724511\n",
      "[161]\ttraining's auc: 0.802142\tvalid_1's auc: 0.724308\n",
      "[162]\ttraining's auc: 0.802432\tvalid_1's auc: 0.724306\n",
      "[163]\ttraining's auc: 0.80267\tvalid_1's auc: 0.724327\n",
      "[164]\ttraining's auc: 0.803117\tvalid_1's auc: 0.724537\n",
      "[165]\ttraining's auc: 0.8034\tvalid_1's auc: 0.7246\n",
      "[166]\ttraining's auc: 0.803743\tvalid_1's auc: 0.724534\n",
      "Early stopping, best iteration is:\n",
      "[156]\ttraining's auc: 0.800593\tvalid_1's auc: 0.724649\n",
      "lightgbm  train  auc: 0.7246489160243519\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "param = dict()\n",
    "param['objective'] = 'binary'\n",
    "param['boosting_type'] = 'gbdt'\n",
    "param['metric'] = 'auc'\n",
    "param['verbose'] = 0\n",
    "param['learning_rate'] = 0.1\n",
    "param['max_depth'] = -1\n",
    "param['feature_fraction'] = 0.8\n",
    "param['bagging_fraction'] = 0.8\n",
    "param['bagging_freq'] = 1\n",
    "param['num_leaves'] = 15\n",
    "param['min_data_in_leaf'] = 64\n",
    "param['is_unbalance'] = False\n",
    "param['verbose'] = -1\n",
    "\n",
    "lgb_train = lgb.Dataset(data=X_train,\n",
    "                        label=y_train,\n",
    "                        )\n",
    "lgb_test = lgb.Dataset(data=X_test,\n",
    "                       label=y_test,\n",
    "                       )\n",
    "model = lgb.train(param,\n",
    "                  lgb_train,\n",
    "                  early_stopping_rounds=10,\n",
    "                  num_boost_round=1000,\n",
    "                  valid_sets=[lgb_train, lgb_test],\n",
    "                  verbose_eval=1)\n",
    "y_pred = model.predict(X_test)\n",
    "print('lightgbm  train  auc:',roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbdt  train  auc: 0.7148537897162462\n"
     ]
    }
   ],
   "source": [
    "# GBDT model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(learning_rate=0.3,n_estimators=40,max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "print('gbdt  train  auc:',roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression  train  auc: 0.6800681508856634\n"
     ]
    }
   ],
   "source": [
    "# LR model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C = 0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "print('logistic regression  train  auc:',roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression  train  auc: 0.601178235428526\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def func_logisticregression(X, y):\n",
    "    '''\n",
    "    Classification algorithm.\n",
    "    Input:  X: Training sample features, P-by-N\n",
    "            y: Training sample labels, 1-by-N\n",
    "    Output: w: learned logistic regression parameters, (P+1)-by-1\n",
    "    '''\n",
    "    P, N = X.shape\n",
    "    w = np.ones((P+1, 1))\n",
    "    # generate matrix\n",
    "    iter_num = 400\n",
    "    alpha = 0.001\n",
    "    X = np.row_stack((X,np.ones((1, N))))\n",
    "    for iter in range(iter_num):\n",
    "        for i in range(N):\n",
    "            x_i = X[:, i].reshape(P+1, 1)\n",
    "            sig = sigmoid(np.dot(w.T,x_i))\n",
    "            error = y[i] - sig\n",
    "            w +=  alpha * x_i * error\n",
    "    return w\n",
    "\n",
    "# w0 + w1*y1 + w2*y2 + w464*y464 = y\n",
    "w = func_logisticregression(X_train.T,y_train.T)\n",
    "test_label = np.ones((15000,1))\n",
    "test_data = np.column_stack((X_test,test_label)).T\n",
    "y_pred = np.sign(np.dot(w.T, test_data)).reshape(-1,1)\n",
    "\n",
    "print('logistic regression  train  auc:',roc_auc_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "from xgboost import XGBClassifier\n",
    "model = GradientBoostingClassifier(learning_rate=0.3,n_estimators=50,max_depth=5)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "print('xgboost regression  train  auc:',roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_predict = final_test.loc[:,final_test.columns =='id']\n",
    "y_evaluate_label = model.predict_proba(test)[:, 1]\n",
    "final_predict['Y'] = y_evaluate_label\n",
    "final_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predict.to_csv(\"3180102099_pre.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
